{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0zMZbZl5taLj"
      },
      "outputs": [],
      "source": [
        "# Importing required modules\n",
        "import requests\n",
        "import pdfplumber\n",
        "import tempfile\n",
        "import openai\n",
        "import tiktoken\n",
        "from rouge_score import rouge_scorer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZ6IqNedtcWE",
        "outputId": "3fd93657-50bc-48c0-da83-c76039c1c19a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A Survey on Dialogue Systems:\n",
            "Recent Advances and New Frontiers\n",
            "Hongshen Chen†, Xiaorui Liu‡, Dawei Yin†, and Jiliang Tang‡\n",
            "†DataScienceLab,JD.com\n",
            "‡DataScienceandEngineering Lab,MichiganStateUniversity\n",
            "chenhongshen@jd.com, yindawei@acm.org,{xiaorui,tangjili}@msu.edu\n",
            "8102\n",
            "ABSTRACT\n",
            "May I know\n",
            "Dialogue systems have attracted more and more attention. your name ?\n",
            "Dialogue State\n",
            "Recent advances on dialogue systems are overwhelmingly NLU Tracking\n",
            "naJ\n",
            "contributed by deep learning techniques, which have been\n",
            "employed to enhance a wide range of big data applications\n",
            "such as computer vision, natural language processing, and\n",
            "11\n",
            "recommender systems. For dialogue systems, deep learning I am Robot.\n",
            "can leverage a massive amount of data to learn meaningful NLG Policy learning\n",
            "feature representations and response generation strategies,\n",
            "]LC.sc[\n",
            "whilerequiringaminimumamountofhand-crafting. Inthis\n",
            "article, we give an overview to these recent advances on di-\n",
            "Figure 1: Traditional Pipeline for Task-oriented Systems.\n",
            "alogue systems from various perspectives and discuss some\n",
            "possible research directions. In particular, we generally di-\n",
            "vide existing dialogue systems into task-oriented and non-\n",
            "certaintasks(e.g. findingproducts,andbookingaccommo-\n",
            "task-oriented models, then detail how deep learning tech-\n",
            "dationsandrestaurants). Thewidelyappliedapproachesto\n",
            "niqueshelpthemwithrepresentativealgorithmsandfinally task-orientedsystemsaretotreatthedialogueresponseasa 3v13710.1171:viXra\n",
            "discuss some appealing research directions that can bring\n",
            "pipelineasshowninFigure1. Thesystemsfirstunderstand\n",
            "the dialogue system research intoa new frontier.\n",
            "themessagegivenbyhuman,representitasainternalstate,\n",
            "thentakesomeactionsaccordingtothepolicy withrespect\n",
            "1. INTRODUCTION to the dialogue state, and finally the action is transformed\n",
            "Tohaveavirtualassistantorachatcompanionsystemwith toits surface form as anaturallanguage. Though language\n",
            "adequate intelligence has seemed illusive, and might only understanding is processed by statistical models, most de-\n",
            "exist in Sci-Fi movies for a long time. Recently, human- ployed dialogue systems still use manual features or hand\n",
            "computer conversation has attracted increasing attention crafted rules for thestate and action space representations,\n",
            "duetoitspromisingpotentialsandalluringcommercialval- intentdetection,andslot filling. Thisnotonly makesitex-\n",
            "ues. With the development of big data and deep learn- pensive and time-consuming to deploy a real dialogue sys-\n",
            "ing techniques, the goal of creating an automatic human- tem, but also limits its usage to other domains. Recently,\n",
            "computer conversation system, as our personal assistant or many algorithms based on deep learning have been devel-\n",
            "chat companion, is no longer an illusion. On the one hand, oped to alleviate those problems by learning feature rep-\n",
            "nowadays we can easily access “big data” for conversations resentations in a high dimensional distributed fashion and\n",
            "on the Web and we might be able to learn how to respond achieve remarkable improvements in these aspects. In ad-\n",
            "andwhattorespondgiven(almost)anyinputs,whichgreatly dition,thereareattemptstobuildend-to-endtask-oriented\n",
            "allows us to build data-driven, open-domain conversation dialogue systems, which can expand the state space repre-\n",
            "systems between humans and computers. On the other sentationinthetraditionalpipelinesystemsandhelpgener-\n",
            "hand,deeplearningtechniqueshavebeenproventobeeffec- alize dialogues outside the annotated task-specific corpora.\n",
            "tiveincapturingcomplexpatternsinbigdataandhavepow- Non-task-oriented systems interact with human to provide\n",
            "ered numerousresearch fields suchas computervision, nat- reasonableresponsesandentertainment. Typically,theyfo-\n",
            "urallanguageprocessingandrecommendersystems. Hence, cus on conversing with human on open domains. Though\n",
            "a large body of literature has emerged to leverage a mas- non-task-orientedsystemsseemtoperformchit-chat,itdom-\n",
            "sive amount of data via deep learning to advance dialogue inates in manyreal word applications. Asrevealed in[111],\n",
            "systems in many perspectives. nearly 80% utterances are chi-chat messages in the online\n",
            "Accordingtotheapplications,dialoguesystemscanberoughly shopping scenario and handling those queries is closely re-\n",
            "categorized intotwogroups–(1)task-orientedsystemsand latedtouserexperiences. Ingeneral,twomajorapproaches\n",
            "(2) non-task-oriented systems (also known as chat bots). have been developed for non-task-oriented systems – (1)\n",
            "Task-oriented systems aim to assist the user to complete generative methods such as sequence-to-sequence models,\n",
            "which generate proper responses during the conversation;\n",
            "and (2) retrieval-based methods, which learn to select re-\n",
            "sponses from thecurrent conversation from a repository.\n",
            "Sentence show restaurant at New York tomorrow\n",
            "applied in intent detection [15; 84; 112]. In particular, [25]\n",
            "Slots O O O B-desti I-desti B-date\n",
            "Intent FindRestaurant usedconvolutionalneuralnetworks(CNN)toextractquery\n",
            "Domain Order vector representations as features for query classification.\n",
            "TheCNN-basedclassificationframeworkalsoresembled[29]\n",
            "Table1: AnIllustrativeExampleofNaturalLanguageRep- and[74]. Similarapproachesarealsoutilized incategory or\n",
            "resentation. domain classification.\n",
            "Slot filling is another challenging problem for spoken lan-\n",
            "guage understanding. Unlike intent detection, slot filling is\n",
            "Therecentdevelopmentofbigdataanddeeplearningtech-\n",
            "usuallydefinedasasequencelabelingproblem,wherewords\n",
            "niques has greatly advanced both task-oriented and non-\n",
            "in the sentence are assigned with semantic labels. The in-\n",
            "oriented dialogue systems, which has encouraged a huge\n",
            "putisthesentenceconsistingofasequenceofwords,andthe\n",
            "amount of deep learning based researches in dialogue sys-\n",
            "outputisasequenceofslot/conceptIDs,oneforeachword.\n",
            "tems. In this article, we aim to (1) give an overview about\n",
            "[17]and[15]useddeepbeliefnetworks(DBNs),andachieved\n",
            "dialoguesystemsespeciallyrecentadvancesfromdeeplearn-\n",
            "superior results compared to CRF baselines. [51; 115; 66;\n",
            "ing; and (2) discuss possible research directions. The re-\n",
            "113] applied RNN for slot filling. The semantic representa-\n",
            "maining of the article is organized as follows. We review\n",
            "tion generated byNLUis furtherprocessed bythedialogue\n",
            "task-oriented dialogue systems including pipeline and end-\n",
            "management component. A typical dialogue management\n",
            "to-endmethodsinSection2. InSection3,wefirstintroduce\n",
            "componentincludestwostages–dialoguestatetrackingand\n",
            "neuralgenerativemethodsincludingpopularmodelsandhot\n",
            "policy learning.\n",
            "researchtopics;andthendetailtheretrieval-basedmethods.\n",
            "InSection4,weconcludetheworkwithdiscussionsonsome\n",
            "2.1.2 DialogueStateTracking\n",
            "research directions.\n",
            "Tracking dialogue states is the core component to ensure\n",
            "a robust manner in dialog systems. It estimates the users\n",
            "2. TASK ORIENTEDDIALOGUESYSTEMS\n",
            "goal at every turn of the dialogue. A dialogue state H t de-\n",
            "Task-orienteddialoguesystemshavebeenanimportantbranch notes the representation of the dialogue session till time t.\n",
            "of spoken dialogue systems. In this section, we will review This classic state structure is commonly called slot filling\n",
            "pipeline and end-to-end methods for task-oriented dialogue or semantic frame. The traditional methods, which have\n",
            "systems. been widely used in most commercial implementations, of-\n",
            "ten adopt hand-crafted rules to select the most likely re-\n",
            "2.1 Pipeline Methods\n",
            "sult [23]. However, these rule-based systems are prone to\n",
            "The typical structure of a pipeline based task-oriented dia- frequent errors as the most likely result is not always the\n",
            "loguesystemisdemonstratedinFigure1. Itconsistsoffour desired one [101].\n",
            "key components: A statistical dialog system maintains a distribution over\n",
            "multiple hypotheses of the true dialog state, facing with\n",
            "• Language understanding. It is known as natural lan-\n",
            "noisyconditionsandambiguity[117]. InDialogStateTrack-\n",
            "guageunderstanding(NLU),whichparsestheuserut-\n",
            "ingChallenge(DSTC)[100;99],theresultsareintheformof\n",
            "terance into predefinedsemantic slots.\n",
            "aprobabilitydistributionovereachslotforeachturn. Ava-\n",
            "• Dialogue state tracker. It manages the input of each rietyofstatisticalapproaches,includingrobustsetsofhand-\n",
            "turn along with the dialogue history and outputs the crafted rules [93], conditional random fields [36; 35; 63],\n",
            "current dialogue state. maximum entropy models [98] and web-style ranking [101]\n",
            "have emerged in Dialog State Tracking Challenge (DSTC)\n",
            "• Dialogue policy learning. It learns the next action shared tasks.\n",
            "based on current dialogue state.\n",
            "Recently, [26] introduced deep learning in belief tracking.\n",
            "It used a sliding window to output a sequence of probabil-\n",
            "• Natural language generation (NLG). It maps the se-\n",
            "ity distributions over an arbitrary number of possible val-\n",
            "lectedactiontoitssurfaceandgeneratestheresponse.\n",
            "ues. Though it was trained in one domain, it can be easily\n",
            "Inthefollowingsubsections,wewillgivemoredetailsabout transferred to new domains. [58] developed multi-domain\n",
            "each component with thestate-of-the art algorithms. RNNdialog statetrackingmodels. Itfirst usedallthedata\n",
            "available to train a very general belief tracking model, and\n",
            "2.1.1 LanguageUnderstanding\n",
            "thenspecialized thegeneralmodelforeachdomain tolearn\n",
            "Given an utterance, natural language understanding maps the domain-specific behavior. [59] proposed a neural belief\n",
            "itintosemanticslots. Theslotsarepre-definedaccordingto tracker (NBT) to detect the slot-value pairs. It took the\n",
            "differentscenarios. Table1illustratesanexampleofnatural system dialogue acts preceding the user input, the user ut-\n",
            "language representation, where “New York” is the location teranceitself,andasinglecandidateslot-valuepairwhichit\n",
            "specified as slot values, and the domain and intent are also needstomakeadecision about,asinput,andtheniterated\n",
            "specified,respectively. Typically,therearetwotypesofrep- over all candidate slot-value pairs to determine which ones\n",
            "resentations. One is the utterance level category, such as havejust been expressed by theuser.\n",
            "the user’s intent and the utterance category. The other is\n",
            "2.1.3 Policylearning\n",
            "the word-level information extraction such as named entity\n",
            "recognition and slot filling. Conditionedonthestaterepresentationfromthestatetracker,\n",
            "An intent detection is performed to detect the intent of a the policy learning is to generate the next available system\n",
            "user. It classifies the utterance into one of the pre-defined action. Either supervised learning or reinforcement learn-\n",
            "intents. Deep learning techniques have been successively ing can be used to optimize policy learning [14]. Typically,\n",
            "a rule-based agent is employed to warm-start the system where the end user’s feedback is hard to be propagated to\n",
            "[111]. Then, supervised learning is conducted on the ac- eachupstreammodule. Thesecondissueisprocessinterde-\n",
            "tions generated by the rules. In online shopping scenario, pendence. The input of a component is dependent on the\n",
            "if the dialogue state is “Recommendation”, then the “Rec- output of another component. When adapting one compo-\n",
            "ommendation” action is triggered, and the system will re- nenttonewenvironmentorretrainedwithnewdata,allthe\n",
            "trieve products from the product database. If the state is othercomponentsneedtobeadaptedaccordinglytoensure\n",
            "“Comparison”, then the system will compare target prod- a global optimization. Slots and features might change ac-\n",
            "ucts/brands[111]. Thedialoguepolicycanbefurthertrained cordingly. This process requires significant human efforts.\n",
            "end-to-end with reinforcement learning to lead the system Withtheadvanceofend-to-endneuralgenerativemodelsin\n",
            "making policies toward the final performance. [14] applied recentyears,manyattemptshavebeenmadetoconstructan\n",
            "deep reinforcement learning on strategic conversation that end-to-end trainable framework for task-oriented dialogue\n",
            "simultaneously learned the feature representation and dia- systems. Note that more details about neural generative\n",
            "logue policy, the system outperformed several baselines in- models will be discussed when we introduce the non-task-\n",
            "cludingrandom,rule-based,andsupervised-basedmethods. oriented systems. Instead of the traditional pipeline, the\n",
            "end-to-end model uses a single module and interacts with\n",
            "2.1.4 NaturalLanguageGeneration\n",
            "structured external databases. [97] and [7] introduced a\n",
            "Thenaturallanguagegenerationcomponentconvertsanab- network-based end-to-end trainable task-oriented dialogue\n",
            "stract dialogue action into natural language surface utter- system,whichtreateddialoguesystemlearningastheprob-\n",
            "ances. Asnoticedin [78],agoodgeneratorusuallyrelieson lemoflearningamappingfromdialoguehistoriestosystem\n",
            "severalfactors: adequacy,fluency,readability,andvariation. responses, and applied an encoder-decoder model to train\n",
            "ConventionalapproachestoNLGtypicallyperformsentence the whole system. However, the system is trained in a su-\n",
            "planning. It maps input semantic symbols into the inter- pervised fashion – not only does it require a lot of training\n",
            "mediary form representing the utterance such as tree-like data,butit mayalsofail tofindagood policyrobustlydue\n",
            "or template structures, and then converts the intermediate to the lack of exploration of dialogue control in the train-\n",
            "structureintothefinalresponsethroughthesurfacerealiza- ing data. [120] first presented an end-to-end reinforcement\n",
            "tion [90; 79]. learning approach to jointly train dialogue state tracking\n",
            "[94]and[95]introducedneuralnetwork-based(NN)approaches andpolicy learningin thedialoguemanagement in orderto\n",
            "toNLGwithaLSTM-basedstructuresimilarwithRNNLM optimize the system actions more robustly. In the conver-\n",
            "[52]. Thedialogueacttypeanditsslot-valuepairsaretrans- sation, the agent asks the user a series of Yes/Noquestions\n",
            "formed into a 1-hot control vectorand is given as theaddi- to find the correct answer. This approach was shown to be\n",
            "tionalinput,whichensuresthatthegeneratedutterancerep- promising when applied tothetask-oriented dialogue prob-\n",
            "resentstheintendedmeaning. [94]usedaforwardRNNgen- lemofguessingthefamouspeopleusersthinkof. [45]trained\n",
            "erator together with a CNN reranker, and backward RNN theend-to-endsystem asa taskcompletion neuraldialogue\n",
            "reranker. All thesub-modules are jointly optimized to gen- system, where its final goal is to complete a task, such as\n",
            "erate utterances conditioned by the required dialogue act. movie-ticket booking.\n",
            "To address the slot information omitting and duplicating Task-orientedsystemsusually needtoqueryoutsideknowl-\n",
            "problemsinsurfacerealization, [95]usedanadditionalcon- edgebase. Previoussystemsachievedthisbyissuingasym-\n",
            "trol cell to gate the dialogue act. [83] extended this ap- bolic query to the knowledge base to retrieve entries based\n",
            "proach by gating the input token vector of LSTM with the on their attributes, where semantic parsing on the input is\n",
            "dialogue act. It was then extended to the multi-domain performed to construct a symbolic query representing the\n",
            "setting by multiple adaptation steps [96]. [123] adopted beliefs of the agent about the user goal[97; 103; 45]. This\n",
            "an encoder-decoder LSTM-based structure to incorporate approachhastwodrawbacks: (1)theretrievedresultsdonot\n",
            "thequestioninformation,semanticslotvalues,anddialogue carry any information about uncertainty in semantic pars-\n",
            "act type to generate correct answers. It used the attention ing,and(2)theretrievaloperationisnondifferentiable,and\n",
            "mechanismtoattendtothekeyinformation conditionedon hence the parser and dialog policy are trained separately.\n",
            "thecurrentdecodingstateofthedecoder. Encodingthedi- This makes online end-to-end learning from user feedback\n",
            "alogueacttypeembedding,theneuralnetwork-basedmodel difficult once the system is deployed. [21] augmented ex-\n",
            "is able to generate variant answers in response to different isting recurrent network architectures with a differentiable\n",
            "act types. [20] also presented a natural language genera- attention-based key-value retrieval mechanism over the en-\n",
            "tor based on the sequence-to-sequence approach that can tries of a knowledge base, which is inspired by key-value\n",
            "be trained to produce natural language strings as well as memory networks[54]. [18] replaced symbolic queries with\n",
            "deep syntax dependency trees from input dialogue acts. It an induced“soft” posterior distribution overtheknowledge\n",
            "was then extended with the preceding user utterance and base that indicates which entities the user is interested in.\n",
            "responses [19]. It enabled the model entraining (adapting) Integrating the soft retrieval process with a reinforcement\n",
            "to users ways of speaking, which provides contextually ap- learner. [102]combinedanRNNwithdomain-specificknowl-\n",
            "propriate responses. edge encoded as software and system action templates.\n",
            "2.2 End to End Methods\n",
            "3. NON TASK ORIENTEDDIALOGUESYS\n",
            "Despite a lot of domain-specific handcrafting in traditional\n",
            "TEM\n",
            "task oriented dialogue systems, which are diffcult to adapt\n",
            "to new domains [7], [120] further noted that, the conven- Unlike task-oriented dialogue systems, which aim to com-\n",
            "tional pipeline of task-oriented dialogue systems has two plete specific tasks for user, non-task-oriented dialogue sys-\n",
            "main limitations. One is the credit assignment problem, tems(alsoknownaschatbots) focusonconversingwithhu-\n",
            "Decoder\n",
            "Givenasourcesequence(message)X =(x1,x2,...,x T)con-\n",
            "sisting of T words and a target sequence (response) Y =\n",
            "y y y y ′\n",
            "1 2 3 T` (y1,y2,...,y T′)oflengthT ,themodelmaximizesthegenera-\n",
            "tionprobabilityofY conditionedonX: p(y1,...,y T′|x1,...,x T).\n",
            "Specifically,asequence-to-sequencemodel(orSeq2Seq)isin\n",
            "anencoder-decoderstructure. Figure2isageneralillustra-\n",
            "(cid:536)(cid:3) tion of such structure. The encoder reads X word by word\n",
            "and represents it as a context vector c through a recurrent\n",
            "neural network (RNN),and then the decoder estimates the\n",
            "generationprobabilityofY withcastheinput. Theencoder\n",
            "Context\n",
            "RNNcalculates thecontext vector c by\n",
            "h =f(x t,h t−1),\n",
            "t\n",
            "(cid:536)(cid:3)\n",
            "whereh t isthehiddenstateattimestept,f isanon-linear\n",
            "function suchas long-short term memory unit (LSTM) [27]\n",
            "and gated recurrent unit (GRU) [12], and c is the hidden\n",
            "x x x state corresponding to the last word h T. The decoder is\n",
            "1 2 T\n",
            "a standard RNN language model with an additional condi-\n",
            "Encoder tional context vector c. The probability distribution p t of\n",
            "candidate words at every time t is calculated as\n",
            "Figure 2: An Illustration of theEncoder-Decoder Model. s t =f(y t−1,s t−1,c),\n",
            "p =softmax(s t,y t−1),\n",
            "t\n",
            "man on open domains [64]. In general, chat bots are im- where s t is the hidden state of the decoder RNN at time t\n",
            "andy t1isthewordattimet1intheresponsesequence. The\n",
            "plemented either by generative methods or retrieval-based\n",
            "objective function of Seq2Seqis definedas:\n",
            "methods. Generativemodelsareabletogeneratemoreproper\n",
            "responses that could have never appeared in the corpus, ′\n",
            "T\n",
            "whileretrieval-basedmodelsenjoytheadvantageofinforma-\n",
            "p((y1,...,y T′|x1,...,x T)=p(y1|c)Yp(y t|c,y1,...,y t−1).\n",
            "tive and fluent responses[30], because they select a proper\n",
            "t=2\n",
            "responseforthecurrentconversationfromarepositorywith\n",
            "response selection algorithms. In the following sections, we [5] then improved the performance by the attention mech-\n",
            "will first dive into the neural generative models, one of the anism, where each word in Y is conditioned on different\n",
            "most popular research topics in recent years, and discuss context vector c, with the observation that each word in Y\n",
            "their drawbacks and possible improvements. Then, we in- may relate to different parts in x. In particular, y i corre-\n",
            "troduce recent advances of deep learning in retrieval based sponds to a context vector c i, and c i is a weighted average\n",
            "models. of theencoder hidden states h1,...,h T:\n",
            "c =ΣT j=1α ijh j,\n",
            "3.1 Neural GenerativeModels i\n",
            "Nowadays, a large amount of conversational exchanges is where α i,j is computed by:\n",
            "available insocial mediawebsites suchasTwitter andRed- exp(e ij)\n",
            "α= ,\n",
            "dit, which raise the prospect of building data-driven mod- ΣT exp(e )\n",
            "k=1 ik\n",
            "els. [64]proposedagenerativeprobabilisticmodel,whichis\n",
            "e =g(s t−1,h j),\n",
            "basedonphrase-basedStatisticalMachineTranslation[118], ij\n",
            "tomodelconversationsonmicro-blogging. Itviewedthere- where g is a multilayer perceptron.\n",
            "sponse generation problem as a translation problem, where [71] applied the recurrent neural network encoder-decoder\n",
            "a post needs to be translated into a response. However, framework[12]togenerateresponsesonTwitter-stylemicro-\n",
            "generatingresponseswasfoundtobeconsiderablymoredif- blogging websites, while [87] utilized a similar model de-\n",
            "ficultthantranslatingbetweenlanguages. Itislikelydueto scribed in [50]. In general, these models utilize neural net-\n",
            "thewiderangeofplausibleresponsesandthelackofphrase workstorepresentdialoguehistoriesandtogenerateappro-\n",
            "alignmentbetweenthepostandtheresponse. Thesuccessof priate responses. Such models are able to leverage a large\n",
            "applyingdeeplearninginmachinetranslation,namelyNeu- amount of data in order to learn meaningful natural lan-\n",
            "ralMachineTranslation,spurstheenthusiasmofresearches guage representations and generation strategies, while re-\n",
            "in neural generative dialogue systems. quiringaminimumamountofdomainknowledgeandhand-\n",
            "Inthefollowingsubsections,wefirstintroducethesequence- crafting.\n",
            "to-sequencemodels,thefoundationofneuralgenerativemod-\n",
            "els. Then, we discuss hot research topics in the direction 3.1.2 DialogueContext\n",
            "including incorporating dialogue context,improving there- Theabilitytotakeintoaccountpreviousutterancesiskeyto\n",
            "sponse diversity, modeling topics and personalities, lever- building dialog systems that can keep conversations active\n",
            "aging outside knowledge base, the interactive learning and and engaging. [77] addressed the challenge of the context\n",
            "evaluation. sensitiveresponsegenerationbyrepresentingthewholedia-\n",
            "logue history (including the current message ) with contin-\n",
            "3.1.1 Sequence to SequenceModels\n",
            "uous representations or embeddings of words and phrases.\n",
            "Theresponseisgenerated asRNNlanguagemodel[52],the byfirst sampling an assignment of thelatent variables, and\n",
            "same as the decoder in [12]. [68] used hierarchical mod- thendecodingdeterministically. [11]presentedalatentvari-\n",
            "els,firstcapturingthemeaningofindividualutterancesand able model for one-shot dialogue response. The model con-\n",
            "then integrating them as discourses. [109] extended the hi- tained a stochastic component z in the decoder P(Y|z,X),\n",
            "erarchical structurewiththeattentionmechanism [5]toat- where z is computed following the variational auto-encoder\n",
            "tend to important parts within and among utterances with framework[34; 33; 75]. [69] introduced latent variables to\n",
            "word level attention and utterance level attention, respec- the hierarchical dialogue modeling framework [68]. The la-\n",
            "tively. [82] conducted a systematic comparison among ex- tent variable is designed to make high-level decisions like\n",
            "isting methods (including non-hierarchical and hierarchical topic or sentiment. [73] conditioned the latent variable on\n",
            "models) and proposed a variant that weights the context explicit attributes to make the latent variable more inter-\n",
            "withrespecttocontext-queryrelevance. Itfoundthat(1)hi- pretable. These attributes can be either manually assigned\n",
            "erarchicalRNNsgenerallyoutperformnon-hierarchicalones, or automatically detected such topics, and personality.\n",
            "and (2) with context information, neural networks tend to\n",
            "3.1.4 TopicandPersonality\n",
            "generate longer, more meaningful and diversereplies.\n",
            "Learning the inherent attributes of dialogues explicitly is\n",
            "3.1.3 ResponseDiversity another way to improve the diversity of dialogues and en-\n",
            "A challenging problem in current sequence-to-sequencedia- surestheconsistency. Amongdifferentattributes,topicand\n",
            "logue systems is that they tend to generate trivial or non- personality are widely explored.\n",
            "committal, universally relevant responses with little mean- [108]noticedthatpeopleoftenassociatetheirdialogueswith\n",
            "ing, which are often involving high frequency phrases along topicallyrelatedconceptsandcreatetheirresponsesaccord-\n",
            "the lines of I dont know or Im OK [77; 87; 68]. ing to these concepts. They used Twitter LDA model to\n",
            "Thisbehaviorcanbeascribedtotherelativehighfrequency get the topic of the input, fed topic information and input\n",
            "ofgenericresponseslikeIdontknow inconversationaldatasets, representationintoajointattentionmoduleandgenerateda\n",
            "incontrastwiththerelativesparsityofmoreinformativeal- topic-relatedresponse. Asmallimprovementindecoderhad\n",
            "ternative responses. One promising approach to alleviate achievedabetterresultin[107]. [13]madeamorethorough\n",
            "such challenge is to find a better objective function. [38] generalization of the problem. They classified each utter-\n",
            "pointed out that neural models assign high probability to ance in the dialogue into one domain, and generated the\n",
            "“safe responses when optimizing the likelihood of outputs domain and content of next utteranceaccordingly.\n",
            "given inputs. They used a Maximum Mutual Information [67] jointly modeled the high-level coarse tokens sequence\n",
            "(MMI), which was first introduced in speech recognition [6; and the dialogue generation explicitly, where the coarse to-\n",
            "9], as an optimization objective. It measured the mutual kens sequence aims to exploit high-level semantics. They\n",
            "dependencebetween inputsandoutputs,whereit took into exploited nouns and activity-entity for the coarse sequence\n",
            "consideration the inverse dependency of responses on mes- representation.\n",
            "sages. [114]incorporatedinversedocumentfrequency(IDF) [122]addedemotionembeddingintoagenerativemodeland\n",
            "[65]intothetrainingprocesstomeasuretheresponsediver- achieved good performance in perplexity. [3] enhanced the\n",
            "sity. model of producing emotionally rich responses from three\n",
            "Some researches realized that the decoding process is an- aspects: incorporating cognitive engineered affective word\n",
            "othersourceofredundantcandidateresponses. [86],[72]and embeddings, augmenting the loss objective with an affect-\n",
            "[42]recognizedthatthebeam-search,anapproximateinfer- constrained objective function, and injecting affective dis-\n",
            "ence algorithm to decode output sequences for neural se- similarity in diverse beam-search inference procedure[86].\n",
            "quence models, lacks diversity when generating candidates [61] gave the system an identity with profile so that the\n",
            "inthebeam. [86]augmentedthebeam-searchobjectivewith system can answer personalized question consistently. [39]\n",
            "a dissimilarity term that measured the diversity between furthertooktheinformationofaddresseeintoconsideration\n",
            "candidate sequences. [72] introduced a stochastic beam- to create a more realistic chatbot.\n",
            "search procedure, while [42] added an additional term for Since the training data comes from different speakers with\n",
            "beam search scoring to penalize the siblings–expansions of inconsistency,[119]proposedatwo-phasetrainingapproach\n",
            "the same parent node in the search. [38; 77; 72] further which initialized the model using large scale data and then\n",
            "performed a re-ranking step with global features to avoid fine-tunedthemodeltogeneratepersonalizedresponse. [55]\n",
            "generating dull or generic responses. [57] conjectured that used transfer reinforcement learning to eliminate inconsis-\n",
            "notonlytheproblemliesintheobjectiveofdecodingandre- tencies.\n",
            "sponsefrequency,butalso themessage itself maylacksuffi-\n",
            "3.1.5 OutsideKnowledgeBase\n",
            "cientinformationforthereplay. Itproposedtousepointwise\n",
            "mutual information (PMI) to predict a noun as a keyword, An important distinction between human conversation and\n",
            "reflecting the main gist of the reply, and then generates a dialogue system is whether it is combined with reality. In-\n",
            "reply containing thegiven keyword. corporatinganoutsideKnowledgeBase(KB)isapromising\n",
            "Another series of works have focused on generating more approach to bridge the gap of background knowledge be-\n",
            "diverse outputs by introducing a stochastic latent variable. tween a dialogue system and human.\n",
            "They demonstrated that natural dialogue is not determin- Memory network is a classic method dealing with question\n",
            "istic – replies for a same message may vary from person to answering tasks with knowledge base. Thus, it is quite\n",
            "person. However,currentresponseissampled from adeter- straightforwardtoapplyitindialoguegeneration. [22]made\n",
            "ministic encoder-decoder model. By incorporating a latent attemptsontop ofthisandhasachievedgood performance\n",
            "variable,thesemodelshavetheadvantagethat,atthegener- in open-domain conversations. [88] also worked on open-\n",
            "ationtime,theycansamplearesponsefromthedistribution domain conversations with background knowledge by cou-\n",
            "plingCNNembeddingandRNNembeddingintomultimodal [80]proposed tousetwoneural networkmodels toevaluate\n",
            "space and made progress in perplexity. A similar task is a sequence of turn-levelfeatures to rate the success of a di-\n",
            "to generate an answer for a question according to outside alogue. [47]encoded thecontext,thetrueresponseand the\n",
            "knowledge. Unlike the general method of tuple retrieval in candidate response into vector representations using RNN,\n",
            "knowledge base, [116] used words from knowledge base to- andthencomputedascoreusingadot-productbetweenthe\n",
            "getherwithcommonwordsingenerationprocess. Empirical vectorsinalinearlytransformedspace. [81]combinedrefer-\n",
            "studies demonstrated that theproposed model was capable encedandunreferencedmetrics,wheretheformermeasured\n",
            "of generating natural and right answers tothequestions by the similarity between reply and the groundtruth through\n",
            "referring to thefacts in theknowledge base. word embedding, and the latter scored the correlation be-\n",
            "tweenreplyandquerytrainedwithamax-marginobjective\n",
            "3.1.6 InteractiveDialoguelearning function, where the negative reply is randomly sampled.\n",
            "Learning through interaction is one of the ultimate goals OnepromisingapproachcomesfromtheideaofTuringtest[85]–\n",
            "of dialogue systems. [43] simulated dialogues between two employing an evaluator to distinguish machine-generated\n",
            "virtual agents. They defined simple heuristic approxima- texts from human-generated ones. [32] and [10] explored\n",
            "tionstorewardsthatcharacterizegoodconversations: good adversarialevaluationmodel[8]whichassignsascorebased\n",
            "conversations are forward-looking [1] or interactive (a turn on how easy it is to distinguish the generated responses\n",
            "suggests a following turn), informative, and coherent. The from humanresponses, while[44]directly applied adversar-\n",
            "parametersofanencoder-decoderRNNdefinedapolicyover ial learning[24; 16] intodialogue generation.\n",
            "aninfiniteaction spaceconsistingofallpossibleutterances.\n",
            "3.2 Retrieval basedMethods\n",
            "The agent learned a policy by optimizing the long-term\n",
            "developer-definedrewardfromongoingdialoguesimulations Retrieval-based methods choose a response from candidate\n",
            "using policy gradient methods [104], rather than the MLE responses. The key to retrieval-based methods is message-\n",
            "objective defined in the standard SEQ2SEQ models. [40] response matching. Matching algorithms have to overcome\n",
            "furtherattemptedtoimprovethebot’sabilitytolearnfrom semantic gaps between messages and responses [28].\n",
            "interaction. By using policy learning and forward predic-\n",
            "3.2.1 Single turnResponseMatching\n",
            "tiononbothtextualandnumericalfeedback,themodelcan\n",
            "improve itself by interacting with human in a (semi-)online Earlystudiesofretrieval-basedchatbotsmainlyfocusonre-\n",
            "way. Instead ofusinghand-crafted reward functions foron- sponseselectionforsingle-turnconversation[91],whereonly\n",
            "line reinforcement learning, [4] performed online human in- the message is used to select a proper response. Typically,\n",
            "the-loop active learning by repeatedly letting human select thecontextandthecandidateresponseareencodedasavec-\n",
            "one of the K responses, generated by an offline supervised tor respectively, and then the matching score is computed\n",
            "pretrained dialogue agent, as the ’best’ response, and then based on those two vectors. Suppose x is the vector repre-\n",
            "respondtotheselectedresponse. Thenetworkisalsotrained sentation of a message and y corresponds to the response\n",
            "with the joint corss-entropy loss function. vectorrepresentation,thematchingfunctionbetweenxand\n",
            "As most human respondents may ask for clarification or y can be as simply as bilinear matching:\n",
            "hints when not confident about the answer, it is natural to T\n",
            "match(x,y)=x Ay,\n",
            "makethebotowningsuchacapability. [41]definedthreesit-\n",
            "uationswherethebothasproblemsinansweringaquestion. where A is a pre-determined matrix, or more complicated\n",
            "Compared the experimental results of not using a asking- ones. [49]proposed aDNN-basedmatchingmodelforshort\n",
            "questionway,thismethodmadegreatimprovementinsome textsresponseselectionandcombinedthelocalnessandhier-\n",
            "scenarios. [37] explored the task on negotiation dialogues. archyintrinsicinthestructure. [28]improvedthemodelby\n",
            "As conventional sequence-to-sequence models simulate hu- utilizingadeepconvolutionalneuralnetworkarchitectureto\n",
            "man dialoguesbutfailtooptimizeaspecificgoal, thiswork learntherepresentationofmessageandresponse,ordirectly\n",
            "took a goal-oriented training and decoding approach and learntheinteractedrepresentationoftwosentences,followed\n",
            "demonstrated a worthwhile perspective. byamulti-layerperceptrontocomputethematchingscore.\n",
            "[92] extracted dependencytree matchingpatterns and used\n",
            "3.1.7 Evaluation\n",
            "themassparseone-hotinputsofadeepfeed-forwardneural\n",
            "Evaluating the quality of the generated response is an im- network for context-response matching. [105] incorporated\n",
            "portant aspect of dialogue response generation systems[46]. thetopicvectorgeneratedbyTwitterLDAmodel[121]into\n",
            "Task-oriented dialogue system can be evaluated based on the CNN based structure to boost responses with rich con-\n",
            "human-generated supervised signals, such as a task com- tent.\n",
            "pletion test or a user satisfaction score[89; 56; 31], how-\n",
            "3.2.2 Multi turnResponseMatching\n",
            "ever, automatically evaluating the quality of generated re-\n",
            "sponses for non-task-oriented dialogue systems remains an Inrecentyears,multi-turnretrieval-basedconversationdraws\n",
            "open question due to the high response diversity [2]. De- more and more attention. In multi-turn response selection,\n",
            "spitethefactthatwordoverlapmetricssuchasBLEU,ME- currentmessageandpreviousutterancesaretakenasinput.\n",
            "TEOR,andROUGEhavebeenwidely usedtoevaluatethe The model selects a response that is natural and relevant\n",
            "generated responses, [46] found that those metrics, as well to the whole context. It is important to identify important\n",
            "as word embedding metrics derived from word embedding information in previous utterances and properly model the\n",
            "models such as Word2Vec[53] haveeither weak or nocorre- utterances relationships to ensure conversation consistency.\n",
            "lation with human judgements, although word embedding [48]encodedthecontext(aconcatenationofallpreviousut-\n",
            "metrics are able to significantly distinguish between base- terancesandcurrentmessage)andcandidateresponseintoa\n",
            "lines and state-of-the-art models across multiple datasets. contextvectorandaresponsevectorthroughaRNN/LSTM\n",
            "basedstructure,respectively,andthencomputedthematch- at leveraginglarge amountofdata. Weneednewway\n",
            "ing degree score based on those two vectors. [110] selected tobridgeoverthewarm-upstage. Itispromisingthat\n",
            "thepreviousutterancesindifferentstrategiesandcombined the dialogue agent has the ability to learn by itself\n",
            "themwithcurrentmessagestoformareformulatedcontext. from the interactions with human.\n",
            "[124] performed context-response matching on not only the\n",
            "generalwordlevelcontextvectorbutalsotheutterancelevel • Deep Understanding. Current neural network based\n",
            "context vector. [106] further improved theleveraging of ut- dialogue systems heavily rely on the huge amount of\n",
            "terances relationship and contextualinformation bymatch- differenttypesofannotateddata,andstructuredknowl-\n",
            "ing a response with each utterancein thecontext on multi- edgebaseand conversationdata. Theylearn tospeak\n",
            "plelevelsofgranularitywithaconvolutionalneuralnetwork, by imitating a response again and again, just like an\n",
            "and then accumulated the vectors in a chronological order infant,andtheresponsesarestilllackofdiversityand\n",
            "through a recurrent neural network to model relationships sometimes are not meaningful. Hence, the dialogue\n",
            "among utterances. agent should be able to learn more effectively with a\n",
            "deepunderstandingofthelanguageandtherealworld.\n",
            "3.3 HybridMethods\n",
            "Specifically, it remains much potential if a dialogue\n",
            "Combing neural generative and retrieval based models can agent can learn from human instruction to get rid of\n",
            "havesignificanteffectsonperformance.[76]and[62]attempted repeatedly training. Since a great quantity of knowl-\n",
            "to combine both methods. Retrieval-based systems often edgeis available on theInternet,adialogue agent can\n",
            "give precise but blunt answers, while generation-based sys- besmarterifitiscapableofutilizingsuchunstructured\n",
            "temstendtogivefluentbutmeaninglessresponses. Inanen- knowledge resource to make comprehension. Last but\n",
            "semble model, the retrieved candidate, along with the orig- notleast,adialogueagentshouldbeabletomakerea-\n",
            "inal message, are fed to an RNN-based response generator. sonableinference,findsomethingnew,shareitsknowl-\n",
            "The final response is given by a post-reranker. This ap- edge across domains, instead of repeating the words\n",
            "proachcombinedtheadvantagesofretrievalandgeneration like a parrot.\n",
            "based models, which was appealing in performance. [70]\n",
            "integrated natural language generation and retrieval mod- • Privacy Protection. Widely applied dialogue system\n",
            "els, including template-based models, bag-of-words mod- serves a large number of people. It is quite necessary\n",
            "els,sequence-to-sequenceneuralnetworkandlatentvariable tonoticethefact that weareusingthesamedialogue\n",
            "neuralnetworkmodelsandappliedreinforcementlearningto assistant. With the ability of learning through inter-\n",
            "crowdsourceddataandreal-worlduserinteractionstoselect actions, comprehension and inference, a dialogue as-\n",
            "an appropriate response from themodels in its ensemble. sistant can inadvertently and implicitly store some of\n",
            "sensitive information [60]. Hence, it is important to\n",
            "4. DISCUSSIONAND CONCLUSION protect users’ privacy while building better dialogue\n",
            "systems.\n",
            "Deeplearninghasbecomeabasictechniqueindialoguesys-\n",
            "tems. Researchersinvestigatedonapplyingneuralnetworks\n",
            "Acknowledgements\n",
            "to the different components of a traditional task-oriented\n",
            "dialogue system,includingnaturallanguageunderstanding,\n",
            "XiaoruiLiuandJiliangTangaresupportedbytheNational\n",
            "naturallanguagegeneration,dialoguestatetracking. Recent\n",
            "ScienceFoundation(NSF)undergrantnumberIIS-1714741\n",
            "years, end-to-end frameworks become popular in not only\n",
            "and IIS-1715940.\n",
            "the non-task-oriented chit-chat dialogue systems, but also\n",
            "the task-oriented ones. Deep learning is capable of leverag-\n",
            "5. REFERENCES\n",
            "inglargeamountofdataandispromisingtobuildupauni-\n",
            "fiedintelligentdialoguesystem. Itisblurringtheboundaries\n",
            "between the task-oriented dialogue systems and non-task- [1] J. Allwood, J. Nivre, and E. Ahls´en. On the seman-\n",
            "oriented systems. In particular, the chit-chat dialogues are tics and pragmatics of linguistic feedback. Journal of\n",
            "modeled by the sequence-to-sequence model directly. The semantics, 9(1):1–26, 1992.\n",
            "task completion models arealso movingtowards an end-to-\n",
            "endtrainablestylewithreinforcementlearningrepresenting [2] R. Artstein, S. Gandhe, J. Gerten, A. Leuski, and\n",
            "the state-action space and combing thewhole pipelines. D. Traum. Semi-formal evaluation of conversational\n",
            "It is worth noting that current end-to-end models are still characters. pages 22–35, 2009.\n",
            "far from perfect. Despite theaforementioned achievements,\n",
            "the problems remain challenging. Next, we discuss some [3] N.Asghar,P.Poupart,J.Hoey,X.Jiang,andL.Mou.\n",
            "possible research directions: Affective neural response generation. arXiv preprint\n",
            "arXiv:1709.03968, 2017.\n",
            "• Swift Warm-Up. Although end-to-end models have\n",
            "drawn most of the recent research attention, we still [4] N. Asghar, P. Poupart, X.Jiang, and H. Li. Deep ac-\n",
            "need to rely on traditional pipelines in practical dia- tive learning for dialogue generation. In Proceedings\n",
            "logue engineering, especially in a new domain warm- of the 6th Joint Conference on Lexical and Computa-\n",
            "up stage. The daily conversation data is quite “big”, tional Semantics (* SEM 2017), pages 78–83, 2017.\n",
            "however, the dialogue data for a specific domain is\n",
            "quite limited. In particular, domain specific dialogue [5] D.Bahdanau,K.Cho,andY.Bengio.Neuralmachine\n",
            "data collection and dialogue system construction are translation by jointly learning to align and translate.\n",
            "laborsome. Neural network based models are better arXiv preprint arXiv:1409.0473, 2014.\n",
            "[6] L.Bahl,P.Brown,P.DeSouza,andR.Mercer.Maxi- [18] B. Dhingra, L. Li, X. Li, J. Gao, Y.-N. Chen,\n",
            "mummutualinformationestimationofhiddenmarkov F.Ahmed,andL.Deng.Towardsend-to-endreinforce-\n",
            "model parameters for speech recognition. In Acous- ment learning of dialogue agents for information ac-\n",
            "tics, Speech, and Signal Processing, IEEE Interna- cess.InProceedings of the 55th Annual Meeting of the\n",
            "tional Conference on ICASSP’86., volume 11, pages Association forComputational Linguistics(Volume1:\n",
            "49–52. IEEE, 1986. Long Papers), pages 484–495, Vancouver, Canada,\n",
            "July2017.Association forComputationalLinguistics.\n",
            "[7] A. Bordes, Y. L. Boureau, and J. Weston. Learning\n",
            "[19] O. Duˇsek and F. Jurcicek. A context-aware natural\n",
            "end-to-endgoal-oriented dialog. In ICLR,2017.\n",
            "language generator for dialogue systems. In Proceed-\n",
            "ingsofthe17thAnnualMeetingoftheSpecialInterest\n",
            "[8] S. R. Bowman, L. Vilnis, O. Vinyals, A. M. Dai,\n",
            "Group onDiscourse andDialogue,pages185–190, Los\n",
            "R. Jozefowicz, and S. Bengio. Generating sentences\n",
            "Angeles, September 2016. Association for Computa-\n",
            "from a continuous space. In Proceedings of The 20th\n",
            "tional Linguistics.\n",
            "SIGNLL Conference on Computational Natural Lan-\n",
            "guage Learning. [20] O. Duˇsek and F. Jurcicek. Sequence-to-sequencegen-\n",
            "eration for spoken dialogue via deep syntax trees and\n",
            "[9] P. F. Brown. The acoustic-modeling problem in\n",
            "strings. In Proceedings of the 54th Annual Meeting of\n",
            "automatic speech recognition. Technical report,\n",
            "the Association for Computational Linguistics (Vol-\n",
            "CARNEGIE-MELLON UNIV PITTSBURGH PA\n",
            "ume 2: Short Papers), pages 45–51, Berlin, Germany,\n",
            "DEPT OFCOMPUTER SCIENCE, 1987.\n",
            "August 2016. Association for Computational Linguis-\n",
            "tics.\n",
            "[10] E. Bruni and R. Ferna´ndez. Adversarial evaluation\n",
            "for open-domain dialogue generation. In Proceedings [21] M. Eric and C. D. Manning. Key-value retrieval\n",
            "of the 18th Annual SIGdial Meeting on Discourse and networks for task-oriented dialogue. arXiv preprint\n",
            "Dialogue, pages 284–288, 2017. arXiv:1705.05414, 2017.\n",
            "[22] M. Ghazvininejad, C. Brockett, M.-W. Chang,\n",
            "[11] K.Cao andS.Clark. Latentvariabledialogue models\n",
            "B. Dolan, J. Gao, W.-t. Yih, and M. Galley. A\n",
            "andtheirdiversity.InProceedings of the 15th Confer-\n",
            "knowledge-groundedneuralconversationmodel.arXiv\n",
            "ence of the European Chapter of the Association for\n",
            "preprint arXiv:1702.01932, 2017.\n",
            "Computational Linguistics: Volume 2, Short Papers,\n",
            "pages 182–187, Valencia, Spain, April 2017. Associa- [23] D. Goddeau, H. Meng, J. Polifroni, S. Seneff, and\n",
            "tion for Computational Linguistics. S. Busayapongchai. A form-based dialogue manager\n",
            "forspokenlanguageapplications.InSpokenLanguage,\n",
            "[12] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bah- 1996. ICSLP 96. Proceedings., Fourth International\n",
            "danau, F. Bougares, H. Schwenk, and Y. Bengio. Conference on,volume2,pages701–704. IEEE,1996.\n",
            "Learning phrase representations using rnn encoder–\n",
            "decoder for statistical machine translation. In Pro- [24] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,\n",
            "ceedings of the 2014 Conference on Empirical Meth- D. Warde-Farley, S. Ozair, A. Courville, and Y. Ben-\n",
            "ods in Natural Language Processing (EMNLP), pages gio. Generative adversarial nets. In Advances in neu-\n",
            "1724–1734, Doha, Qatar, October 2014. Association ral information processing systems, pages 2672–2680,\n",
            "for Computational Linguistics. 2014.\n",
            "[25] H.B.Hashemi,A.Asiaee,andR.Kraft.Queryintent\n",
            "[13] S. Choudhary, P. Srivastava, L. Ungar, and J. Sedoc.\n",
            "detection using convolutional neural networks. In In-\n",
            "Domain aware neural dialog system. arXiv preprint\n",
            "ternational Conference onWebSearch andDataMin-\n",
            "arXiv:1708.00897, 2017.\n",
            "ing, Workshop on Query Understanding, 2016.\n",
            "[14] H. Cuayhuitl, S. Keizer, and O. Lemon. Strategic di- [26] M.Henderson,B.Thomson,andS.Young.Deepneu-\n",
            "alogue management via deep reinforcement learning. ralnetworkapproachforthedialogstatetrackingchal-\n",
            "arxiv.org, 2015. lenge. In Proceedings of the SIGDIAL 2013 Confer-\n",
            "ence, pages 467–471, 2013.\n",
            "[15] L. Deng, G. Tur, X. He, and D. Hakkani-Tur. Use of\n",
            "kernel deep convex networks and end-to-end learning [27] S. Hochreiter and J. Schmidhuber. Long short-term\n",
            "for spoken language understanding. In Spoken Lan- memory. Neural computation, 9(8):1735–1780, 1997.\n",
            "guageTechnology Workshop(SLT),2012IEEE,pages\n",
            "[28] B.Hu,Z.Lu,H.Li,andQ.Chen.Convolutionalneu-\n",
            "210–215. IEEE, 2012.\n",
            "ral network architectures for matching natural lan-\n",
            "guage sentences. In Advances in neural information\n",
            "[16] E.L.Denton,S.Chintala,R.Fergus,etal.Deepgen-\n",
            "processing systems, pages 2042–2050, 2014.\n",
            "erativeimagemodelsusingalaplacian pyramidofad-\n",
            "versarialnetworks.InAdvances inneural information [29] P.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, and\n",
            "processing systems, pages 1486–1494, 2015. L. Heck. Learning deep structured semantic models\n",
            "forwebsearchusingclickthroughdata.InProceedings\n",
            "[17] A.DeorasandR.Sarikaya.Deepbeliefnetworkbased of the 22nd ACM international conference on Confer-\n",
            "semantic taggers for spoken language understanding. ence on information & knowledge management, pages\n",
            "In Interspeech, pages 2713–2717, 2013. 2333–2338. ACM, 2013.\n",
            "[30] Z. Ji, Z. Lu, and H. Li. An information retrieval [43] J. Li, W. Monroe, A. Ritter, D. Jurafsky, M. Galley,\n",
            "approach to short text conversation. arXiv preprint and J. Gao. Deep reinforcement learning for dialogue\n",
            "arXiv:1408.6988, 2014. generation. In Proceedings of the 2016 Conference on\n",
            "Empirical Methods in Natural Language Processing,\n",
            "[31] C. Kamm. User interfaces for voice applications. pages 1192–1202, Austin, Texas, November 2016. As-\n",
            "Proceedings of the National Academy of Sciences, sociation for Computational Linguistics.\n",
            "92(22):10031–10037, 1995.\n",
            "[44] J. Li, W. Monroe, T. Shi, S. Jean, A. Ritter, and\n",
            "[32] A. Kannan and O. Vinyals. Adversarial evaluation D. Jurafsky. Adversarial learning for neural dialogue\n",
            "of dialogue models. arXiv preprint arXiv:1701.08198, generation. In Proceedings of the 2017 Conference on\n",
            "2017. Empirical Methods in Natural Language Processing,\n",
            "pages 2147–2159, Copenhagen, Denmark, September\n",
            "[33] D. P. Kingma, D. J. Rezende, S. Mohamed, and 2017. Association for Computational Linguistics.\n",
            "M. Welling. Semi-supervised learning with deep gen-\n",
            "erative models. Advances in Neural Information Pro- [45] X.Li,Y.-N.Chen,L.Li,andJ.Gao.End-to-endtask-\n",
            "cessing Systems, 4:3581–3589, 2014. completion neural dialogue systems. arXiv preprint\n",
            "arXiv:1703.01008, 2017.\n",
            "[34] D. P. Kingma and M. Welling. Auto-encoding varia-\n",
            "[46] C.-W. Liu, R. Lowe, I. Serban, M. Noseworthy,\n",
            "tional bayes. In ICLR,2014.\n",
            "L. Charlin, and J. Pineau. How not to evaluate your\n",
            "dialogue system: An empirical study of unsupervised\n",
            "[35] S. Lee. Structured discriminative model for dialog\n",
            "state tracking. In SIGDIAL Conference, pages 442– evaluation metrics for dialogue response generation.\n",
            "In Proceedings of the 2016 Conference on Empirical\n",
            "451, 2013.\n",
            "Methods inNatural Language Processing, pages2122–\n",
            "[36] S. Lee and M. Eskenazi. Recipe for building robust 2132, Austin, Texas, November 2016. Association for\n",
            "spoken dialog state trackers: Dialog state tracking Computational Linguistics.\n",
            "challengesystemdescription.InSIGDIALConference,\n",
            "[47] R. Lowe, M. Noseworthy, I. V. Serban, N. Angelard-\n",
            "pages 414–422, 2013.\n",
            "Gontier, Y. Bengio, and J. Pineau. Towards an auto-\n",
            "matic turing test: Learning to evaluate dialogue re-\n",
            "[37] M. Lewis, D. Yarats, Y. Dauphin, D. Parikh, and\n",
            "sponses.InProceedings of the 55th Annual Meeting of\n",
            "D.Batra. Deal or no deal? end-to-endlearning of ne-\n",
            "the Association for Computational Linguistics (Vol-\n",
            "gotiation dialogues. In Proceedings of the 2017 Con-\n",
            "ume 1: Long Papers), pages 1116–1126, Vancouver,\n",
            "ference on Empirical Methods in Natural Language\n",
            "Canada, July 2017. Association for Computational\n",
            "Processing, pages 2433–2443, Copenhagen, Denmark,\n",
            "Linguistics.\n",
            "September 2017. Association for Computational Lin-\n",
            "guistics.\n",
            "[48] R. Lowe, N. Pow, I. Serban, and J. Pineau. The\n",
            "ubuntu dialogue corpus: A large dataset for research\n",
            "[38] J.Li,M.Galley,C.Brockett,J.Gao,andB.Dolan.A\n",
            "in unstructured multi-turn dialogue systems. In Pro-\n",
            "diversity-promotingobjectivefunctionforneuralcon-\n",
            "ceedings of the 16th Annual Meeting of the Special In-\n",
            "versation models. In Proceedings of the 2016 Confer-\n",
            "terest Group on Discourse and Dialogue, pages 285–\n",
            "ence of the North American Chapter of the Associa-\n",
            "294, Prague, Czech Republic, September 2015. Asso-\n",
            "tion for Computational Linguistics: Human Language\n",
            "ciation for Computational Linguistics.\n",
            "Technologies, pages 110–119, San Diego, California,\n",
            "June2016.AssociationforComputationalLinguistics.\n",
            "[49] Z. Lu and H. Li. A deep architecture for matching\n",
            "shorttexts.InInternational ConferenceonNeuralIn-\n",
            "[39] J.Li,M.Galley,C.Brockett,G.Spithourakis,J.Gao,\n",
            "formationProcessingSystems,pages1367–1375, 2013.\n",
            "and B. Dolan. A persona-based neural conversation\n",
            "model. In Proceedings of the 54th Annual Meeting of [50] T. Luong, I. Sutskever, Q. Le, O. Vinyals, and\n",
            "the Association for Computational Linguistics (Vol- W. Zaremba. Addressing the rare word problem in\n",
            "ume 1: Long Papers), pages 994–1003, Berlin, Ger- neuralmachinetranslation.InProceedings of the 53rd\n",
            "many, August 2016. Association for Computational Annual Meeting of the Association for Computational\n",
            "Linguistics. Linguisticsandthe7thInternationalJointConference\n",
            "onNatural Language Processing (Volume1: LongPa-\n",
            "[40] J.Li,A.H.Miller,S.Chopra,M.Ranzato,andJ.We- pers), pages 11–19, Beijing, China, July 2015. Associ-\n",
            "ston.Dialoguelearningwithhuman-in-the-loop.arXiv ation for Computational Linguistics.\n",
            "preprint arXiv:1611.09823, 2016.\n",
            "[51] G. Mesnil, X. He, L. Deng, and Y. Bengio. Investi-\n",
            "[41] J.Li,A.H.Miller,S.Chopra,M.Ranzato,andJ.We- gation of recurrent-neural-network architectures and\n",
            "ston.Learningthroughdialogueinteractionsbyasking learning methods for spoken language understanding.\n",
            "questions. arXiv preprint, 2017. Interspeech, 2013.\n",
            "[42] J. Li, W. Monroe, and J. Dan. A simple, fast di- [52] T. Mikolov, M. Karafi´at, L.Burget, J. Cernocky`,and\n",
            "versedecoding algorithm for neuralgeneration. arXiv S. Khudanpur. Recurrent neural network based lan-\n",
            "preprint arXiv:1611.08562, 2016. guage model. In Interspeech, volume 2, page 3, 2010.\n",
            "[53] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, [63] H. Ren, W. Xu, Y. Zhang, and Y. Yan. Dialog state\n",
            "andJ.Dean.Distributedrepresentationsofwordsand trackingusingconditionalrandomfields.InSIGDIAL\n",
            "phrases and their compositionality. In Advances in Conference, pages 457–461, 2013.\n",
            "neural information processing systems, pages 3111–\n",
            "3119, 2013. [64] A. Ritter, C. Cherry, and W. B. Dolan. Data-driven\n",
            "responsegeneration insocial media.InConference on\n",
            "[54] A. Miller, A. Fisch, J. Dodge, A.-H. Karimi, A. Bor- Empirical Methods in Natural Language Processing,\n",
            "des, and J. Weston. Key-value memory networks for pages 583–593, 2011.\n",
            "directlyreadingdocuments.InProceedingsofthe2016\n",
            "ConferenceonEmpiricalMethodsinNaturalLanguage [65] G.SaltonandC.Buckley.Term-weightingapproaches\n",
            "Processing, pages 1400–1409, Austin, Texas, Novem- in automatic text retrieval. Information Processing &\n",
            "ber2016. Association for Computational Linguistics. Management, 24(5):513–523, 1988.\n",
            "[55] K. Mo, S. Li, Y. Zhang, J. Li, and Q. Yang. Person- [66] R. Sarikaya, G. E. Hinton, and B. Ramabhadran.\n",
            "alizing a dialogue system with transfer reinforcement Deep belief nets for natural language call-routing. In\n",
            "learning. arXiv preprint, 2016. IEEE International Conference on Acoustics, Speech\n",
            "and Signal Processing, pages 5680–5683, 2011.\n",
            "[56] S. M¨oller, R. Englert, K. Engelbrecht, V. Hafner,\n",
            "A. Jameson, A. Oulasvirta, A. Raake, and N. Rei- [67] I. Serban, T. Klinger, G. Tesauro, K. Talamadupula,\n",
            "thinger. Memo: towards automatic usability evalua- B.Zhou,Y.Bengio,andA.Courville.Multiresolution\n",
            "tion of spoken dialogue services by user error simu- recurrentneuralnetworks: Anapplicationtodialogue\n",
            "lations. In Ninth International Conference on Spoken responsegeneration.InAAAIConferenceonArtificial\n",
            "Language Processing, 2006. Intelligence, 2017.\n",
            "[57] L. Mou, Y. Song, R. Yan, G. Li, L. Zhang, and\n",
            "[68] I. Serban, A. Sordoni, Y. Bengio, A. Courville, and\n",
            "Z. Jin. Sequence to backward and forward sequences:\n",
            "J. Pineau. Building end-to-end dialogue systems us-\n",
            "A content-introducing approach to generative short-\n",
            "ing generative hierarchical neural network models. In\n",
            "text conversation. In Proceedings of COLING 2016,\n",
            "AAAI Conference on Artificial Intelligence, 2016.\n",
            "the 26th International Conference on Computational\n",
            "Linguistics: Technical Papers, pages 3349–3358, Os- [69] I.Serban,A.Sordoni,R.Lowe,L.Charlin, J.Pineau,\n",
            "aka, Japan, December 2016. The COLING 2016 Or- A.Courville,andY.Bengio.Ahierarchicallatentvari-\n",
            "ganizing Committee. able encoder-decoder model for generating dialogues.\n",
            "[58] N.Mrkˇsi´c,D.O´ S´eaghdha,B.Thomson,M.Gasic,P.- In AAAI Conference on Artificial Intelligence, 2017.\n",
            "H. Su, D. Vandyke,T.-H. Wen, and S. Young. Multi-\n",
            "[70] I.V.Serban,C.Sankar,M.Germain,S.Zhang,Z.Lin,\n",
            "domain dialog state tracking using recurrent neural\n",
            "S. Subramanian, T. Kim, M. Pieper, S. Chandar,\n",
            "networks. In Proceedings of the 53rd Annual Meet-\n",
            "N.R.Ke,etal.Adeepreinforcementlearningchatbot.\n",
            "ing of the Association for Computational Linguistics\n",
            "arXiv preprint arXiv:1709.02349, 2017.\n",
            "and the 7th International Joint Conference on Nat-\n",
            "ural Language Processing (Volume 2: Short Papers),\n",
            "[71] L.Shang,Z.Lu,andH.Li.Neuralrespondingmachine\n",
            "pages794–799, Beijing, China,July2015.Association\n",
            "forshort-textconversation.InProceedings of the 53rd\n",
            "for Computational Linguistics.\n",
            "Annual Meeting of the Association for Computational\n",
            "[59] N. Mrkˇsi´c, D. O´ S´eaghdha, T.-H. Wen, B. Thomson, Linguisticsandthe7thInternationalJointConference\n",
            "onNatural Language Processing (Volume1: LongPa-\n",
            "and S. Young.Neural belief tracker: Data-driven dia-\n",
            "pers),pages1577–1586, Beijing,China,July2015.As-\n",
            "loguestatetracking.InProceedingsofthe55thAnnual\n",
            "sociation for Computational Linguistics.\n",
            "MeetingoftheAssociationforComputationalLinguis-\n",
            "tics (Volume 1: Long Papers), pages 1777–1788, Van-\n",
            "[72] L. Shao, S. Gouws, D. Britz, A. Goldie, and\n",
            "couver,Canada, July2017. Association for Computa-\n",
            "B. Strope. Generating long and diverse responses\n",
            "tional Linguistics.\n",
            "with neural conversation models. arXiv preprint\n",
            "[60] N.Papernot, M. Abadi, U´.Erlingsson, I. Goodfellow, arXiv:1701.03185, 2017.\n",
            "andK.Talwar.Semi-supervisedknowledgetransferfor\n",
            "[73] X. Shen, H. Su, Y. Li, W. Li, S. Niu, Y. Zhao,\n",
            "deeplearningfromprivatetrainingdata.ICLR,2017.\n",
            "A. Aizawa, and G. Long. A conditional variational\n",
            "[61] Q. Qian, M. Huang, H. Zhao, J. Xu, and X. Zhu. framework fordialoggeneration.InProceedings of the\n",
            "Assigning personality/identity to a chatting machine 55th Annual Meeting of the Association for Compu-\n",
            "for coherent conversation generation. arXiv preprint tational Linguistics (Volume 2: Short Papers), pages\n",
            "arXiv:1706.02861, 2017. 504–509, Vancouver, Canada, July 2017. Association\n",
            "for Computational Linguistics.\n",
            "[62] M.Qiu,F.-L.Li,S.Wang,X.Gao,Y.Chen,W.Zhao,\n",
            "H. Chen, J. Huang, and W. Chu. Alime chat: A se- [74] Y. Shen, X. He, J. Gao, L. Deng, and G. Mesnil.\n",
            "quence to sequence and rerank based chatbot engine. Learningsemanticrepresentationsusingconvolutional\n",
            "In Proceedings of the 55th Annual Meeting of the As- neural networks for web search. In Proceedings of the\n",
            "sociation for Computational Linguistics (Volume 2: 23rd International Conference on World Wide Web,\n",
            "Short Papers), volume 2, pages 498–503, 2017. pages 373–374. ACM, 2014.\n",
            "[75] K.Sohn,X.Yan,andH.Lee.Learningstructuredout- [87] O.VinyalsandQ.Le.Aneuralconversational model.\n",
            "put representation using deep conditional generative arXiv preprint arXiv:1506.05869, 2015.\n",
            "models. In International Conference on Neural Infor-\n",
            "mation Processing Systems, pages 3483–3491, 2015. [88] P.Vougiouklis,J.Hare,andE.Simperl.Aneuralnet-\n",
            "work approach for knowledge-driven response gener-\n",
            "[76] Y. Song, R. Yan, X. Li, D. Zhao, and M. Zhang. ation. In Proceedings of COLING 2016, the 26th In-\n",
            "Two are better than one: An ensemble of retrieval- ternational Conference on Computational Linguistics:\n",
            "and generation-based dialog systems. arXiv preprint TechnicalPapers,pages3370–3380,Osaka,Japan,De-\n",
            "arXiv:1610.07149, 2016. cember2016.TheCOLING2016OrganizingCommit-\n",
            "tee.\n",
            "[77] A. Sordoni, M. Galley, M. Auli, C. Brockett, Y. Ji,\n",
            "M. Mitchell, J.-Y. Nie, J. Gao, and B. Dolan. A neu-\n",
            "[89] M. A. Walker, D. J. Litman, C. A. Kamm, and\n",
            "ral network approach to context-sensitive generation\n",
            "A. Abella. Paradise: A framework for evaluating spo-\n",
            "ofconversationalresponses.InProceedingsofthe2015\n",
            "ken dialogue agents. In Proceedings of the eighth con-\n",
            "Conference of the North American Chapter of the As-\n",
            "ference on European chapter of the Association for\n",
            "sociationforComputational Linguistics: HumanLan-\n",
            "Computational Linguistics, pages 271–280. Associa-\n",
            "guageTechnologies,pages196–205, Denver,Colorado,\n",
            "tion for Computational Linguistics, 1997.\n",
            "May–June 2015. Association for Computational Lin-\n",
            "guistics. [90] M. A.Walker, O. C. Rambow, and M. Rogati. Train-\n",
            "ingasentenceplannerforspokendialogueusingboost-\n",
            "[78] A.Stent,M.Marge, andM.Singhai.Evaluatingeval-\n",
            "ing. Computer Speech & Language, 16(3):409–433,\n",
            "uationmethodsforgeneration inthepresenceofvari-\n",
            "2002.\n",
            "ation. In International Conference on Computational\n",
            "LinguisticsandIntelligentTextProcessing,pages341– [91] H.Wang,Z.Lu,H.Li,andE.Chen.Adatasetforre-\n",
            "351, 2005. search on short-text conversations. In Proceedings of\n",
            "the 2013 Conference onEmpirical Methods inNatural\n",
            "[79] A. Stent, R. Prasad, and M. Walker. Trainable sen-\n",
            "Language Processing, pages 935–945, Seattle, Wash-\n",
            "tence planning for complex information presentation\n",
            "ington, USA, October 2013. Association for Compu-\n",
            "in spoken dialog systems. In Proceedings of the 42nd\n",
            "tational Linguistics.\n",
            "annual meeting on association for computational lin-\n",
            "guistics, page 79. Association for Computational Lin-\n",
            "[92] M.Wang,Z.Lu,H.Li,andQ.Liu.Syntax-baseddeep\n",
            "guistics, 2004.\n",
            "matching of short texts.In IJCAI,03 2015.\n",
            "[80] P.-H. Su, D. Vandyke, M. Gasic, D. Kim, N. Mrksic,\n",
            "[93] Z. Wang and O. Lemon. A simple and generic belief\n",
            "T.-H. Wen, and S. Young. Learning from real users:\n",
            "trackingmechanismforthedialogstatetrackingchal-\n",
            "Ratingdialoguesuccesswithneuralnetworksforrein-\n",
            "lenge: Onthebelievabilityofobservedinformation.In\n",
            "forcement learning in spoken dialogue systems. arXiv\n",
            "SIGDIAL Conference, pages 423–432, 2013.\n",
            "preprint arXiv:1508.03386, 2015.\n",
            "[94] T.-H. Wen, M. Gasic, D. Kim, N. Mrksic, P.-H. Su,\n",
            "[81] C. Tao, L. Mou, D. Zhao, and R. Yan. Ru-\n",
            "D.Vandyke,andS.Young.Stochasticlanguagegener-\n",
            "ber: An unsupervised method for automatic evalu-\n",
            "ationindialogueusingrecurrentneuralnetworkswith\n",
            "ation of open-domain dialog systems. arXiv preprint\n",
            "convolutionalsentencereranking.InProceedingsofthe\n",
            "arXiv:1701.03079, 2017.\n",
            "16th Annual Meeting of the Special Interest Group on\n",
            "[82] Z. Tian, R. Yan, L. Mou, Y. Song, Y. Feng, and DiscourseandDialogue,pages275–284,Prague,Czech\n",
            "D. Zhao. How to make context more useful? an em- Republic, September 2015. Association for Computa-\n",
            "pirical study on context-aware neural conversational tional Linguistics.\n",
            "models. In Meeting of the Association for Computa-\n",
            "tional Linguistics, pages 231–236, 2017. [95] T.-H. Wen, M. Gasic, N. Mrkˇsi´c, P.-H. Su,\n",
            "D. Vandyke, and S. Young. Semantically conditioned\n",
            "[83] V. K. Tran and L. M. Nguyen. Semantic refinement lstm-basednaturallanguagegeneration forspokendi-\n",
            "gru-based neural language generation for spoken dia- aloguesystems.InProceedings ofthe2015Conference\n",
            "logue systems. In PACLING, 2017. onEmpiricalMethodsinNaturalLanguageProcessing,\n",
            "pages 1711–1721, Lisbon, Portugal, September 2015.\n",
            "[84] G.Tur,L.Deng,D.Hakkani-Tu¨r,andX.He.Towards\n",
            "Association for Computational Linguistics.\n",
            "deeper understanding: Deep convex networks for se-\n",
            "mantic utterance classification. In Acoustics, Speech\n",
            "[96] T.-H. Wen, M. Gaˇsi´c, N. Mrkˇsi´c, L. M. Rojas-\n",
            "andSignalProcessing(ICASSP),2012IEEEInterna-\n",
            "Barahona,P.-H.Su,D.Vandyke,andS.Young.Multi-\n",
            "tional Conference on, pages 5045–5048. IEEE, 2012.\n",
            "domainneuralnetworklanguagegenerationforspoken\n",
            "[85] A.M.Turing.Computingmachineryandintelligence. dialogue systems. In Proceedings of the 2016 Confer-\n",
            "Mind,59(236):433–460, 1950. ence of the North American Chapter of the Associa-\n",
            "tion for Computational Linguistics: Human Language\n",
            "[86] A. K. Vijayakumar, M. Cogswell, R. R. Selvaraju, Technologies, pages 120–129, San Diego, California,\n",
            "Q. Sun, S. Lee, D. Crandall, and D. Batra. Diverse June2016.AssociationforComputationalLinguistics.\n",
            "beam search: Decoding diverse solutions from neu-\n",
            "ralsequencemodels.arXivpreprintarXiv:1610.02424, [97] T.-H. Wen, D. Vandyke, N. Mrkˇsi´c, M. Gasic, L. M.\n",
            "2016. Rojas Barahona, P.-H. Su, S. Ultes, and S. Young. A\n",
            "network-based end-to-end trainable task-oriented di- [110] R. Yan, Y. Song, and H. Wu. Learning to respond\n",
            "alogue system. In Proceedings of the 15th Conference with deep neural networks for retrieval-based human-\n",
            "of the European Chapter of the Association for Com- computer conversation system. In Proceedings of the\n",
            "putational Linguistics: Volume 1, Long Papers, pages 39th International ACM SIGIR Conference on Re-\n",
            "438–449, Valencia, Spain, April 2017. Association for search and Development in Information Retrieval, SI-\n",
            "Computational Linguistics. GIR ’16, pages 55–64, New York, NY, USA, 2016.\n",
            "ACM.\n",
            "[98] J. Williams. Multi-domain learning and generaliza-\n",
            "tionindialogstatetracking.InSIGDIALConference,\n",
            "[111] Z. Yan, N. Duan, P. Chen, M. Zhou, J. Zhou, and\n",
            "pages 433–441, 2013.\n",
            "Z. Li. Building task-oriented dialogue systems for on-\n",
            "lineshopping.InAAAIConferenceonArtificialIntel-\n",
            "[99] J. Williams, A. Raux, D. Ramachandran, and\n",
            "ligence, 2017.\n",
            "A.Black. Thedialog statetrackingchallenge. InPro-\n",
            "ceedingsoftheSIGDIAL2013Conference,pages404–\n",
            "[112] D.Yann,G.Tur,D.Hakkani-Tur,andL.Heck.Zero-\n",
            "413, 2013.\n",
            "shot learning and clustering for semantic utterance\n",
            "[100] J. D. Williams. A belief tracking challenge task for classification using deep learning. 2014.\n",
            "spokendialog systems.InNAACL-HLT Workshop on\n",
            "Future Directions and Needs in the Spoken Dialog [113] K. Yao, B. Peng, Y. Zhang, D. Yu, G. Zweig, and\n",
            "Community: Tools and Data, pages 23–24, 2012. Y. Shi. Spoken language understanding using long\n",
            "short-term memory neural networks. In IEEE Insti-\n",
            "[101] J. D. Williams. Web-style ranking and slu combina-\n",
            "tute ofElectrical &Electronics Engineers,pages189–\n",
            "tionfordialogstatetracking.InSIGDIALConference,\n",
            "194, 2014.\n",
            "pages 282–291, 2014.\n",
            "[114] K.Yao,B.Peng,G.Zweig,andK.F.Wong.Anatten-\n",
            "[102] J. D. Williams, K. Asadi, and G. Zweig. Hybrid code\n",
            "tionalneuralconversationmodelwithimprovedspeci-\n",
            "networks: practical and efficient end-to-end dialog\n",
            "ficity.arXiv preprint arXiv:1606.01292, 2016.\n",
            "control with supervised and reinforcement learning.\n",
            "In Proceedings of the 55th Annual Meeting of the As-\n",
            "[115] K. Yao, G. Zweig, M. Y. Hwang, Y. Shi, and D. Yu.\n",
            "sociation for Computational Linguistics (Volume 1:\n",
            "Recurrent neural networks for language understand-\n",
            "Long Papers), pages 665–677, Vancouver, Canada,\n",
            "ing. In Interspeech, 2013.\n",
            "July2017.Association forComputationalLinguistics.\n",
            "[103] J. D. Williams and G. Zweig. End-to-end lstm-based [116] J. Yin, X. Jiang, Z. Lu, L. Shang, H. Li, and X. Li.\n",
            "dialog control optimized with supervised and rein- Neural generative question answering. In Proceedings\n",
            "forcement learning. arXiv preprint arXiv:1606.01269, of the Twenty-Fifth International Joint Conference\n",
            "2016. on Artificial Intelligence, IJCAI’16, pages 2972–2978.\n",
            "AAAIPress, 2016.\n",
            "[104] R.J.Williams.Simplestatisticalgradient-followingal-\n",
            "gorithmsforconnectionistreinforcementlearning.Ma-\n",
            "[117] S. Young, M. Gai, S. Keizer, F. Mairesse, J. Schatz-\n",
            "chine learning, 8(3-4):229–256, 1992.\n",
            "mann, B. Thomson, and K. Yu. The hidden informa-\n",
            "tion state model: A practical framework for pomdp-\n",
            "[105] Y.Wu,W.Wu,Z.Li,andM.Zhou.Topicaugmented\n",
            "based spoken dialogue management. 24(2):150–174,\n",
            "neural network for short text conversation. CoRR,\n",
            "2010.\n",
            "2016.\n",
            "[106] Y. Wu, W. Wu, C. Xing, M. Zhou, and Z. Li. Se- [118] R.Zens,F.J.Och,andH.Ney.Phrase-basedstatisti-\n",
            "quential matching network: A new architecture for calmachinetranslation.InGermanConferenceonAi:\n",
            "multi-turn response selection in retrieval-based chat- Advances in Artificial Intelligence,pages18–32, 2002.\n",
            "bots.InProceedings ofthe55thAnnualMeetingofthe\n",
            "AssociationforComputational Linguistics(Volume1: [119] W.Zhang, T. Liu,Y.Wang,and Q.Zhu.Neuralper-\n",
            "Long Papers), pages 496–505, Vancouver, Canada, sonalized response generation as domain adaptation.\n",
            "July2017.Association forComputationalLinguistics. arXiv preprint arXiv:1701.02073, 2017.\n",
            "[107] C. Xing, W. Wu, Y. Wu, J. Liu, Y. Huang, M. Zhou,\n",
            "[120] T. Zhao and M. Eskenazi. Towards end-to-end learn-\n",
            "and W. Y. Ma. Topic augmented neural response\n",
            "ing for dialog state tracking and management us-\n",
            "generation with a joint attention mechanism. arXiv\n",
            "ing deepreinforcement learning. In Proceedings of the\n",
            "preprint arXiv:1606.08340, 2016. 17th Annual Meeting of the Special Interest Group\n",
            "on Discourse and Dialogue, pages 1–10, Los Angeles,\n",
            "[108] C. Xing, W. Wu, Y. Wu, J. Liu, Y. Huang, M. Zhou,\n",
            "September 2016. Association for Computational Lin-\n",
            "and W.-Y. Ma. Topic aware neural response genera-\n",
            "guistics.\n",
            "tion. In AAAI Conference on Artificial Intelligence,\n",
            "2017.\n",
            "[121] W. X. Zhao, J. Jiang, J. Weng, J. He, E. P. Lim,\n",
            "[109] C. Xing, W. Wu, Y. Wu, M. Zhou, Y. Huang, H.Yan,andX.Li.Comparing twitterandtraditional\n",
            "and W. Y. Ma. Hierarchical recurrent attention media using topic models. In European Conference\n",
            "network for response generation. arXiv preprint on Advances inInformationRetrieval,pages338–349,\n",
            "arXiv:1701.07149, 2017. 2011.\n",
            "[122] H. Zhou, M. Huang, T. Zhang, X. Zhu, and B. Liu.\n",
            "Emotional chatting machine: Emotional conversation\n",
            "generation with internal and external memory. arXiv\n",
            "preprint arXiv:1704.01074, 2017.\n",
            "[123] H. Zhou, M. Huang, and X. Zhu. Context-aware nat-\n",
            "urallanguagegenerationforspokendialoguesystems.\n",
            "In COLING,pages 2032–2041, 2016.\n",
            "[124] X. Zhou, D. Dong, H. Wu, S. Zhao, D. Yu, H. Tian,\n",
            "X. Liu, and R. Yan. Multi-view response selection\n",
            "for human-computer conversation. In Proceedings of\n",
            "the2016 Conference onEmpirical Methods inNatural\n",
            "Language Processing, pages 372–381, Austin, Texas,\n",
            "November 2016. Association for Computational Lin-\n",
            "guistics.\n"
          ]
        }
      ],
      "source": [
        "# Download the PDF\n",
        "url = \"https://arxiv.org/pdf/1711.01731\"\n",
        "response = requests.get(url)\n",
        "\n",
        "# Create a temporary file to store the PDF (NOTE: When uploaded directly as a .pdf file has issue with extraction regardless of the library utilized)\n",
        "with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as temp_pdf:\n",
        "    temp_pdf.write(response.content)\n",
        "    temp_pdf_path = temp_pdf.name\n",
        "\n",
        "# Extracting the text\n",
        "all_text = []\n",
        "with pdfplumber.open(temp_pdf_path) as pdf:\n",
        "    for page in pdf.pages:\n",
        "        all_text.append(page.extract_text())\n",
        "\n",
        "# Combine results\n",
        "all_text_combined = \"\\n\".join(filter(None, all_text))\n",
        "\n",
        "# Output the extracted text\n",
        "print(all_text_combined)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QQVFdjSmt0aQ"
      },
      "outputs": [],
      "source": [
        "# Declaring an API key\n",
        "openai.api_key = \"your_key\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jzPuxaUGt4mY"
      },
      "outputs": [],
      "source": [
        "# Define the chunking function\n",
        "\n",
        "def chunking(text, chunkSize, overlap):\n",
        "\n",
        "    # Determine the token limit for our model and perform encoding\n",
        "    tt_encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    tokens = tt_encoding.encode(text)\n",
        "    total_tokens = len(tokens)\n",
        "\n",
        "    chunks = []\n",
        "\n",
        "    # Perform chunking based on token limitations\n",
        "    for i in range(0, total_tokens, chunkSize - overlap):\n",
        "        chunk = tokens[i:i+chunkSize]\n",
        "        chunks.append(chunk)\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WZLl6BNXt62Q"
      },
      "outputs": [],
      "source": [
        "# Run the chunking() on our text\n",
        "chunks = chunking(all_text_combined, 3000, 50)\n",
        "\n",
        "# Redefine the encoding model for GPT4\n",
        "tt_encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "# Run a prompt and store output of gpt-4 model for each chunk to the final_response list\n",
        "final_response = []\n",
        "\n",
        "for index, chunk in enumerate(chunks):\n",
        "    response = openai.ChatCompletion.create(model = \"gpt-4\", messages = [{\"role\": \"system\", \"content\": \"You are an assistant specialized in summarizing documents.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Please summarize the following document:\\n\\n{tt_encoding.decode(chunk)}\"}, {\"role\": \"assistant\", \"content\": \"Summary:\"}], temperature = 0.2)\n",
        "    final_response.append(response[\"choices\"][0][\"message\"][\"content\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9BZ1ydlt98o",
        "outputId": "c9771c7e-dcb9-4826-e0ad-32b788cca3cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['This document is a survey on dialogue systems, focusing on recent advances and potential future research directions. Dialogue systems, which facilitate human-computer conversation, have gained increasing attention due to their potential applications and commercial value. The recent advances in these systems are largely contributed by deep learning techniques, which have been used to enhance big data applications such as computer vision, natural language processing, and recommender systems. The document categorizes dialogue systems into task-oriented and non-task-oriented models, and discusses how deep learning techniques can enhance them. It also highlights some appealing research directions that could further advance dialogue system research.',\n",
              " 'The document discusses various approaches to Natural Language Generation (NLG) and dialogue systems. It mentions the use of neural network-based approaches with LSTM-based structures similar to RNNLM. The dialogue act type and its slot-value pairs are transformed into a 1-hot control vector. The document also discusses the use of an end-to-end reinforcement learning approach to train dialogue state tracking and policy learning in dialogue management. It also mentions the use of an encoder-decoder LSTM-based structure to incorporate question information, semantic slot values, and dialogue act type to generate correct answers. The document further discusses the limitations of traditional task-oriented dialogue systems and the use of end-to-end methods. It also talks about non-task-oriented dialogue systems, also known as chatbots, and their implementation either by generative methods or retrieval-based methods. The document also discusses the use of sequence-to-sequence models, the incorporation of dialogue context, improving response diversity, modeling topics and personalities, leveraging outside knowledge base, interactive learning, and evaluation.',\n",
              " 'The document discusses the application of deep learning in dialogue systems. It highlights that some research has found the decoding process to be a source of redundant candidate responses. Several studies have focused on generating more diverse outputs by introducing a stochastic latent variable. The document also discusses the importance of incorporating an outside Knowledge Base (KB) to bridge the gap of background knowledge between a dialogue system and human. It also emphasizes the importance of interactive dialogue learning and the evaluation of the quality of generated responses. The document concludes by discussing the potential of deep learning in dialogue systems, including the need for privacy protection.',\n",
              " 'The document discusses the development of a unified intelligent dialogue system that combines task-oriented and non-task-oriented systems. The system uses a sequence-to-sequence model for chit-chat dialogues and reinforcement learning for task completion models. However, the current end-to-end models are not perfect and face several challenges. The document suggests possible research directions, including the need for traditional pipelines in practical dialogue engineering, especially in the initial stages of a new domain. It also highlights the limitations of domain-specific dialogue data and the labor-intensive process of system construction.',\n",
              " 'The document discusses various research papers and studies related to computational linguistics, specifically focusing on neural conversation models and dialogue systems. It covers topics such as persona-based neural conversation models, dialogue learning with human-in-the-loop, learning through dialogue interactions by asking questions, and data-driven response generation in social media. It also discusses the use of recurrent neural networks for dialog state tracking, deep reinforcement learning chatbots, and the use of deep conditional generative models for learning structured output representation. The document also mentions various conferences and meetings where these studies were presented.',\n",
              " 'The document is a collection of references from various research papers and studies related to computational linguistics, artificial intelligence, and dialogue systems. The topics covered include syntax-based deep matching of short texts, reinforcement learning in spoken dialogue systems, semantic refinement, natural language generation, semantic utterance classification, and more. The studies also explore various methods and models such as LSTM-based dialog control, hybrid code networks, sequential matching network, and topic augmented neural response generation.']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Displaying our summaries\n",
        "final_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PF5o74UzuMQI",
        "outputId": "52ac9391-3eb7-4f91-c094-4e058a5c0e31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE FOR ABSTRACT\n",
            "ROUGE-1: Score(precision=0.16129032258064516, recall=0.75, fmeasure=0.26548672566371684)\n",
            "ROUGE-L: Score(precision=0.10906298003072197, recall=0.5071428571428571, fmeasure=0.17951959544879897)\n",
            "ROUGE FOR FULL TEXT\n",
            "ROUGE-1: Score(precision=0.9109062980030722, recall=0.06619781201160974, fmeasure=0.12342595483400978)\n",
            "ROUGE-L: Score(precision=0.8924731182795699, recall=0.06485822728287564, fmeasure=0.12092829638880218)\n",
            "ROUGE FOR ABSTRACT PLUS CONCLUSION\n",
            "ROUGE-1: Score(precision=0.5483870967741935, recall=0.5560747663551402, fmeasure=0.5522041763341067)\n",
            "ROUGE-L: Score(precision=0.25806451612903225, recall=0.2616822429906542, fmeasure=0.259860788863109)\n"
          ]
        }
      ],
      "source": [
        "# Combine the responses into a single string\n",
        "final_response_combined = ' '.join(final_response)\n",
        "\n",
        "# Initialize a ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeLsum'], use_stemmer = True)\n",
        "\n",
        "# String variable for storing an actual abstract from the article\n",
        "abstract = \"Dialogue systems have attracted more and more attention. Recent advances in dialogue systems are overwhelmingly contributed by deep learning techniques, which have been employed to enhance a wide range of big data applications such as computer vision, natural language processing, and recommender systems. For dialogue systems, deep learning can leverage a massive amount of data to learn meaningful feature reprsentations and response generation strategies, while requiring a minimum amount of hand-crafting. In this article, we give an overview to these recent advances on dialogue systems from various perspectives and discuss some possible research directions. In particular, we generally divide existing dialogue systems into task-oriented and non-task oriented models, then detail how deep learning techniques help them with representative algorithms and finally discuss some appealing research directions that can bring the dialogue system research into a new frontier.\"\n",
        "\n",
        "# String variable for storing the conclusion\n",
        "conclusion=\"Deep learning has become a basic technique in dialogue systems. Researchers investigated on applying neural networks to the different components of a traditional task-oriented dialogue system, including natural language understanding, natural language generation, dialogue state tracking. Recent years, end-to-end frameworks become popular in not only the non-task-oriented chit-chat dialogue systems, but also the task-oriented ones. Deep learning is capable of leveraging large amount of data and is promising to build up a unified intelligent dialogue system. It is blurring the boundaries between the task-oriented dialogue systems and non-task- oriented systems. In particular, the chit-chat dialogues are modeled by the sequence-to-sequence model directly. The task completion models are also moving towards an end-to- end trainable style with reinforcement learning representing the state-action space and combing the whole pipelines. It is worth noting that current end-to-end models are still far from perfect. Despite the aforementioned achievements, the problems remain challenging. Next, we discuss some possible research directions: Swift Warm-Up. Although end-to-end models have drawn most of the recent research attention, we still need to rely on traditional pipelines in practical dialogue engineering, especially in a new domain warm- up stage. The daily conversation data is quite “big”, however, the dialogue data for a specific domain is quite limited. In particular, domain specific dialogue data collection and dialogue system construction are laborsome. Neural network based models are better at leveraging large amount of data. We need new way to bridge over the warm-up stage. It is promising that the dialogue agent has the ability to learn by itself from the interactions with human. Deep Understanding. Current neural network based dialogue systems heavily rely on the huge amount of different types of annotated data, and structured knowledge base and conversation data. They learn to speak by imitating a response again and again, just like an infant, and the responses are still lack of diversity and sometimes are not meaningful. Hence, the dialogue agent should be able to learn more effectively with a deep understanding of the language and the real world. Specifically, it remains much potential if a dialogue agent can learn from human instruction to get rid of repeatedly training. Since a great quantity of knowledge is available on the Internet, a dialogue agent can be smarter if it is capable of utilizing such unstructured knowledge resource to make comprehension. Last but not least, a dialogue agent should be able to make rea- sonable inference, find something new, share its knowledge across domains, instead of repeating the words like a parrot. Privacy Protection. Widely applied dialogue system serves a large number of people. It is quite necessary to notice the fact that we are using the same dialogue assistant. With the ability of learning through inter- actions, comprehension and inference, a dialogue assistant can inadvertently and implicitly store some of sensitive information. Hence, it is important to protect users’ privacy while building better dialogue systems.\"\n",
        "\n",
        "# Combine abstract and conclusion\n",
        "abstractonclusion = abstract + conclusion\n",
        "\n",
        "# Calculate the scores\n",
        "scores_abstract = scorer.score(abstract, final_response_combined)\n",
        "scores_text = scorer.score(all_text_combined, final_response_combined)\n",
        "scores_abstractconclusion = scorer.score(abstractonclusion, final_response_combined)\n",
        "\n",
        "# Display scores for article abstract as reference versus the entire article text as reference\n",
        "print(\"ROUGE FOR ABSTRACT\")\n",
        "print(f\"ROUGE-1: {scores_abstract['rouge1']}\")\n",
        "print(f\"ROUGE-L: {scores_abstract['rougeLsum']}\")\n",
        "\n",
        "print(\"ROUGE FOR FULL TEXT\")\n",
        "print(f\"ROUGE-1: {scores_text['rouge1']}\")\n",
        "print(f\"ROUGE-L: {scores_text['rougeLsum']}\")\n",
        "\n",
        "print(\"ROUGE FOR ABSTRACT PLUS CONCLUSION\")\n",
        "print(f\"ROUGE-1: {scores_abstractconclusion['rouge1']}\")\n",
        "print(f\"ROUGE-L: {scores_abstractconclusion['rougeLsum']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjlFp6Q6v92G"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
